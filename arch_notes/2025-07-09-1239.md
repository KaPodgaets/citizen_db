# Refactoring/Design Plan: MS SQL Data Pipeline Enhancement (v4)

## 1. Executive Summary & Goals
This plan details the necessary steps to refactor the existing data pipeline, address current gaps, and introduce new features. The primary objective is to establish a robust, metadata-driven, and production-ready ELT pipeline exclusively on MS SQL Server.

**Key Goals:**
1.  **Formalize Database Structure:** Implement a complete, version-controlled DDL structure for a multi-layered data warehouse (`raw`, `stage`, `core`, `mart`) and a `meta` schema for pipeline control, all within MS SQL Server.
2.  **Enhance Ingestion & Validation:** Introduce strict data quality gates at the beginning of the pipeline, including source file naming conventions, and column mapping from structurally-validated, centrally-managed, versioned contracts.
3.  **Standardize & Harden a DB-Centric Workflow:** Refactor all pipeline components to use a centralized database connection, adopt a pure database-centric workflow, and implement atomic, transactional data loading to ensure reliability and provide clear, actionable error handling.

## 2. Current Situation Analysis
The project has a well-defined architectural vision and a foundational set of Python scripts for an ELT pipeline. However, several critical components are either missing, incomplete, or misaligned with the final requirements.

-   **Database Ambiguity:** The architecture documents specify MS SQL, but several scripts contain hardcoded PostgreSQL connection strings.
-   **Missing DDLs:** There are no formal SQL DDL scripts to create the required database schemas and tables.
-   **Incomplete Configuration:** The configuration utility is not universally used, leading to hardcoded values.
-   **File-Based Staging:** The current flow relies on intermediate Parquet files instead of a fully database-centric staging process.
-   **Lack of Ingestion Controls:** The `ingest.py` script lacks crucial upfront checks on source file names, contract validity, and column mapping.
-   **Date Handling Gaps:** There is no provision for handling mixed date formats from XLSX sources.

## 3. Proposed Solution / Refactoring Strategy
### 3.1. High-Level Design / Architectural Overview
We will refactor the pipeline to a pure, database-centric ELT workflow operating entirely on MS SQL Server. The flow will be orchestrated using statuses in the `meta.ingestion_log` table. A new `contracts` directory will store a single YAML file per dataset, containing all historical versions of its column mappings. The structure of these YAML files will be formally validated on load.

The new end-to-end data flow will be:
```mermaid
graph TD
    A[Source .xlsx File] -->|1. Filename Validation| B(ingest.py)
    B -->|2. Column Mapping & Date Parsing| B
    subgraph "Contract Handling"
      C[contracts/dataset.yml]
      D[src/models/contracts.py (Pydantic)]
      C -->|Reads File| E(src/utils/parsing.py)
      D -->|Validates Structure| E
      E -->|Returns Mapping| B
    end
    B -->|3. Atomic Bulk Load| F[DB: raw Schema]
    F -->|4. Read Raw Data| G(validate.py)
    H[schemas/*.py (Pandera)] -->|Reads Rules| G
    G -->|5. Bulk Load Validated Data| I[DB: stage Schema]
    I -->|6. Read Staged Data| J(transform.py)
    J -->|7. Apply SCD-2 Logic| K[DB: core Schema]
    K -->|8. Read Core Data| L(publish.py)
    L -->|9. Populate Star Schema| M[DB: mart Schema]

    subgraph "Metadata Driven Control"
        N(DB: meta Schema)
        B -->|Logs Ingestion, Status & Contract Version| N
        G -->|Logs Validation| N
        J -->|Logs Transformation| N
        L -->|Logs Publish| N
    end
```

### 3.2. Key Components / Modules
1.  **`sql/` directory:** To contain all DDL and utility SQL scripts.
2.  **`contracts/` directory:** To store one YAML file per dataset, containing all versioned mappings.
    -   `contracts/citizens.yml`: A single file containing a list of all historical column mappings for the `citizens` dataset.
3.  **`src/models/` directory (New):**
    -   `contracts.py`: Will contain Pydantic models to validate the structure of the YAML contract files. For example:
      ```python
      # src/models/contracts.py
      from pydantic import BaseModel, Field
      from typing import Dict, List
      
      class ContractVersion(BaseModel):
          version: str = Field(pattern=r'^\d{4}-\d{2}-\d{2}$')
          column_mapping: Dict[str, str]

      class ContractFile(BaseModel):
          versions: List[ContractVersion]
      ```
4.  **`src/utils/` modules:**
    -   `config.py`: To be used by all scripts.
    -   **`parsing.py` (New):** Will contain helper functions for parsing dates and for loading and validating contracts using the Pydantic models.

### 3.3. Detailed Action Plan / Phases
---

#### **Phase 1: Foundation and Database Setup**
-   **Objective(s):** Establish the complete MS SQL database structure, centralize configurations, and create versioned, structurally-defined contract files.
-   **Priority:** High

-   **Task 1.1:** Create SQL DDL Scripts for All Schemas
    -   **Rationale/Goal:** To have a version-controlled, repeatable way to set up the database.
    -   **Estimated Effort:** L
    -   **Deliverable/Criteria for Completion:** Idempotent `.sql` files are created in `sql/ddl/` for all schemas.

-   **Task 1.2:** Create Database Utility Scripts
    -   **Rationale/Goal:** To provide developers with a quick way to reset the database.
    -   **Estimated Effort:** S
    -   **Deliverable/Criteria for Completion:** A script `sql/utils/drop_all_objects.sql` is created.

-   **Task 1.3:** Create Pydantic Models for Contracts
    -   **Rationale/Goal:** To enforce a strict, testable structure for all contract files, preventing runtime errors from typos.
    -   **Estimated Effort:** S
    -   **Deliverable/Criteria for Completion:** A file `src/models/contracts.py` is created with Pydantic models defining the expected structure of a contract file.

-   **Task 1.4:** Create Versioned YAML Column Mapping Contracts
    -   **Rationale/Goal:** To externalize and version the mapping logic in a single, manageable file per dataset that conforms to the Pydantic models.
    -   **Estimated Effort:** M
    -   **Deliverable/Criteria for Completion:** A `contracts/` directory is created. For each dataset, a single YAML file (e.g., `citizens.yml`) is created containing a top-level `versions` key with a list of versioned mappings.

-   **Task 1.5:** Refactor All Scripts for Centralized DB Configuration
    -   **Rationale/Goal:** To eliminate hardcoded connection strings.
    -   **Estimated Effort:** M
    -   **Deliverable/Criteria for Completion:** All pipeline scripts are modified to import `get_engine` from `src/utils/db.py`.

---

#### **Phase 2: Implement Enhanced Ingestion and Validation**
-   **Objective(s):** Rebuild the ingestion and validation steps to be robust, use the new database layers, and handle versioned contracts and complex data formats with strong error handling.
-   **Priority:** High

-   **Task 2.1:** Implement Data Parsing Utilities
    -   **Rationale/Goal:** To create reusable, testable functions for handling complex parsing of dates and contracts.
    -   **Estimated Effort:** M
    -   **Deliverable/Criteria for Completion:** A new file `src/utils/parsing.py` is created. It contains:
        1. A `parse_date` function for handling various date formats.
        2. A `load_contract(dataset_name, file_date)` function that:
            - Loads the YAML and **validates its contents against the Pydantic model** from `src/models/contracts.py`. Raises a `ValidationError` on failure.
            - Finds the correct mapping for the given `file_date`.
            - Raises an informative `ValueError` if no suitable contract version is found.

-   **Task 2.2:** Refactor `ingest.py` for Robustness and Error Handling
    -   **Rationale/Goal:** To create a hardened ingestion script that provides clear feedback on failure and ensures data integrity.
    -   **Estimated Effort:** L
    -   **Deliverable/Criteria for Completion:** `ingest.py` is refactored to:
        1.  Validate the input filename against the specified regex pattern.
        2.  Call `parsing.load_contract` within a `try...except` block that gracefully handles:
            - **`FileNotFoundError`:** Prints a message "Contract file for dataset 'xyz' not found. Please create 'contracts/xyz.yml'." and exits.
            - **Pydantic `ValidationError`:** Prints "Contract file 'contracts/xyz.yml' is malformed. Please check its structure." and exits.
            - **`ValueError`:** Prints "No valid contract version found for date YYYY-MM. Please add a version to 'contracts/xyz.yml' that precedes this date." and exits.
        3.  Wrap the database write in a single transaction. If `pandas.to_sql` fails, the transaction is rolled back, 'RAW_LOAD_FAIL' is logged, and the script exits with an error.

-   **Task 2.3:** Refactor `validate.py`
    -   **Rationale/Goal:** To adapt the validation step to read from the `raw` database layer and populate the `stage` layer.
    -   **Estimated Effort:** M
    -   **Deliverable/Criteria for Completion:** `validate.py` is refactored to read from `raw` tables, validate, prevent re-validation, and load valid data into `stage` tables.

---

#### **Phase 3: Adapt Transformation and Orchestration**
-   **Objective(s):** Finalize the pipeline by adapting the transformation and orchestration logic to the new database-centric flow.
-   **Priority:** Medium

-   **Task 3.1:** Refactor `transform.py` and `publish.py`
    -   **Rationale/Goal:** To complete the transition away from file-based staging.
    -   **Estimated Effort:** S
    -   **Deliverable/Criteria for Completion:** The scripts are modified to read their source data from the appropriate database tables (`stage` and `core` respectively).

-   **Task 3.2:** Refactor `run_pipeline.py` Orchestrator
    -   **Rationale/Goal:** To align the master pipeline runner with the new metadata-driven process.
    -   **Estimated Effort:** M
    -   **Deliverable/Criteria for Completion:** `run_pipeline.py` is updated to query `meta` tables for records in specific states and trigger the next appropriate script.

### 3.4. Data Model Changes
-   **`meta` Schema:** `ingestion_log` will include `status` and `contract_version_used (NVARCHAR(50))`.
-   **`raw`, `stage`, `core`, `mart` Schemas:** DDLs will be created to formally define tables with appropriate MS SQL data types. All `raw` columns will be `NVARCHAR(MAX)` to ensure load resilience.

## 4. Key Considerations & Risk Mitigation
### 4.1. Technical Risks & Challenges
-   **Risk:** Invalid Contract File Structure.
    -   **Mitigation:** This is now formally mitigated by using Pydantic models to validate the structure of every contract file upon loading. This provides an immediate, clear failure point for malformed contracts, preventing downstream errors.
-   **Risk:** Transactional Integrity during Raw Load.
    -   **Mitigation:** The `ingest.py` script will wrap the entire `pandas.to_sql` operation in an explicit SQLAlchemy transaction. Any exception during the database write will trigger a rollback, ensuring atomicity.

### 4.2. Dependencies
-   **Internal:** The phases are highly sequential. Phase 1 must be completed before Phase 2.
-   **External:** Requires a running MS SQL Server instance and the `pyodbc` driver.

### 4.3. Non-Functional Requirements (NFRs) Addressed
-   **Maintainability:** Centralized configuration, modular parsing utilities, and externalized, structurally-validated, versioned contracts significantly improve maintainability.
-   **Reliability:** Atomic raw loads, idempotent steps, and explicit, comprehensive error handling for missing or malformed contracts improve reliability.
-   **Usability:** Clear, actionable error messages for common operator errors (missing/malformed contracts, missing versions) improve the system's usability.

## 5. Success Metrics / Validation Criteria
-   The ingestion script fails with a clear, specific error message if the contract YAML file is missing.
-   The ingestion script fails with a clear, specific error message if the contract YAML file is structurally invalid (fails Pydantic validation).
-   The ingestion script fails with a clear, specific error message if a source file's date precedes any available contract version.
-   A failed raw data load is correctly rolled back, leaving the target table unchanged, and the error is logged.
-   The end-to-end pipeline can successfully process a valid source file, resulting in correct data in all data layers.

## 6. Assumptions Made
-   The MS SQL Server instance is available and accessible.
-   The `pyodbc` library and a compatible MS SQL ODBC driver can be installed.
-   The business process for adding a new contract version is to edit the appropriate YAML file and add a new item to the `versions` list.
-   The `YYYY-MM` date in the source filename is a sufficient proxy for the data's date to select a contract version. We will use the 1st of that month for comparison.