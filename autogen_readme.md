# Autogenerated

## Citizen Data Integration Project — **Comprehensive Design & Step-by-Step Requirements**

### 1. Executive Summary

You will build an on-prem, **manual-first but automation-ready** pipeline that ingests monthly (and ad-hoc weekly) Excel files for citizens, welfare and absorption programs, validates their structure/content, integrates them into a historized Core layer, and publishes analytics-ready star-schema tables for Power BI users.

*Tech stack*: **Python (pandas + pandera), MS SQL Server, Git, optional Prefect/Dagster later.**
*Security*: SQL logins or Windows Integrated Auth, no special infra required.

---

### 2. Logical Data Layers

| Layer                       | Folder / SQL Schema                             | Purpose                                                                                              | Retention    |
| --------------------------- | ----------------------------------------------- | ---------------------------------------------------------------------------------------------------- | ------------ |
| **Raw (Land)**              | `data/land/` (XLSX)                             | Immutable copy exactly as received.                                                                  | Forever      |
| **Stage (Clean)**           | `data/stage/clean/` (Parquet)  & `stage` schema | Passed schema/content checks; minimal type coercion. One **version per dataset per month**.          | 24 months    |
| **Core (ODS / Integrated)** | `core` schema                                   | Historized, integrated tables (SCD-2). Keeps all monthly versions; flags latest as `is_current = 1`. | Full history |
| **Mart (Star)**             | `mart` schema                                   | Curated, deduped, BI-friendly star schema (dims & facts).                                            | As needed    |

---

### 3. Folder & Repo Structure

```text
citizen_pipeline/
├── data/
│   ├── land/                  # raw XLSX
│   └── stage/
│       └── clean/             # Parquet
├── schemas/                   # pandera models
├── src/
│   ├── ingest.py              # Land → metadata
│   ├── validate.py            # Schema checks → Parquet
│   ├── transform.py           # Stage → Core (+ Mart)
│   ├── publish.py             # (Re)build Mart only
│   ├── run_pipeline.py        # Smart runner using metadata
│   └── utils/                 # db.py, logging.py, etc.
├── tests/                     # pytest unit + integration
├── sql/                       # DDL for meta tables
└── README.md
```

---

### 4. Metadata & Audit Tables (SQL `meta` schema)

| Table                 | Key Columns                                                                                                 | Role in Pipeline                                                 |
| --------------------- | ----------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------- |
| **`ingestion_log`**   | `file_id, file_name, sha256_hash, rows_loaded, load_ts, loaded_by`                                          | Prevent duplicates; track every landed file.                     |
| **`validation_log`**  | `validation_id, file_id FK, status, issues_json, validated_at`                                              | Store PASS/FAIL & detailed error report.                         |
| **`dataset_version`** | `dataset, version_id (e.g. 2025-07-W3[_v002]), is_current, applied_at, applied_by`                          | Point Core to latest valid version; enable rollback/time-travel. |
| **`etl_audit`**       | `run_id, step_name (ingest/validate/transform/publish), status, duration_ms, started_at, ended_at, file_id` | Full execution heartbeat for dashboards.                         |

---

### 5. ETL Pipeline — Steps & Requirements

| #     | Step / Script                             | Manual Command                                                    | Functional Requirements                                                                                                                                                                                              |
| ----- | ----------------------------------------- | ----------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **1** | **Land** (`ingest.py`)                    | `python ingest.py <path>\citizens_2025-07.xlsx --run`             | \* Copy to `data/land/`.<br>\* Compute SHA-256 & file size.<br>\* Insert into `meta.ingestion_log` (reject dup hash).<br>\* `etl_audit` row with `status='OK'` or `FAIL`.                                            |
| **2** | **Validate** (`validate.py`)              | `python validate.py --file_id 123 --html-report`                  | \* Load XLSX → pandas.<br>\* Run **pandera** schema & content checks.<br>\* On PASS: write Parquet to `data/stage/clean/`.<br>\* Insert into `meta.validation_log`.<br>\* Produce HTML fail-sheet.                   |
| **3** | **Transform** (`transform.py`)            | `python transform.py citizens welfare absorption --month 2025-07` | \* Read latest **PASS** Parquets for each dataset.<br>\* Apply business rules & SCD-2 merges into `core.*` tables.<br>\* Update `meta.dataset_version` (flip `is_current`).<br>\* Write `etl_audit` runtime metrics. |
| **4** | **Publish** (`publish.py`)                | `python publish.py citizens --target-version 2025-07`             | \* Build / refresh **mart** star schema: `dim_citizen`, `dim_phone`, `dim_address`, `fact_welfare`, `fact_absorption`.<br>\* Hide raw tables from Power BI role.<br>\* Optional email/Teams notification.            |
| **5** | **Runner (optional)** (`run_pipeline.py`) | `python run_pipeline.py citizens --mode auto`                     | \* Query metadata to detect new PASS files not yet transformed.<br>\* Chain Steps 3 & 4.<br>\* Enables later cron/Prefect scheduling.                                                                                |

---

### 6. Data Model Highlights

```
core.citizen
  citizen_sk (PK) | citizen_id | dob | gender | ...
  valid_from | valid_to | is_current

core.welfare_raw
  row_id (PK) | citizen_id | benefit_type | amt | row_version_id

mart.dim_citizen         mart.fact_welfare
------------------       -------------------
citizen_key (PK) ◄─────┐ citizen_key (FK)
name_full               benefit_type
dob_yyyy_mm             amt
has_welfare_flag        date_key …
…
```

*Phone & address cleansing rules* live in a transformation sub-module and populate `dim_phone`, setting flags such as `is_invalid`, `is_duplicate`, `is_whatsapp`.

---

### 7. Testing & QA

| Layer      | Test Type         | Tools / Notes                                              |
| ---------- | ----------------- | ---------------------------------------------------------- |
| Utils      | Unit tests        | `pytest`, `pytest-cov`, `ruff`.                            |
| Schemas    | Contract tests    | Feed good/bad DataFrames → expect PASS/FAIL.               |
| Transform  | Integration tests | Load tiny CSV fixtures into temp DB; assert SCD-2 results. |
| End-to-End | Slow/nightly      | Run full CLI on sample files; verify mart row counts.      |

CI pipeline (GitHub Actions) runs **unit + schema** on every PR; full E2E nightly.

---

### 8. Security & Access

* **SQL Auth**: `citizen_admin` (DDL/DML), `etl_runner`, `analyst_ro` (mart only).
* **Windows Auth**: same roles via AD groups.
* Connection strings stored in `.env` (git-ignored).
* Optional Dynamic Data Masking on PII columns for `analyst_ro`.

---

### 9. Operational Policies

| Topic                    | Decision                                                                       |
| ------------------------ | ------------------------------------------------------------------------------ |
| **File Naming**          | `<dataset>_YYYY-MM[_Wn][_v00x].xlsx` (e.g., `citizens_2025-07-W3_v002.xlsx`).  |
| **Restartability**       | All scripts idempotent; use SHA-256 & `version_id` to skip duplicates.         |
| **Performance Target**   | ≤ 10 min to ingest 50 MB file on dev server.                                   |
| **Retention**            | Stage Parquet 24 months; Core & Mart indefinite.                               |
| **Rollback**             | `transform.py --rollback <version_id>` drops last load and flips `is_current`. |
| **Automation Readiness** | Each CLI exposes a `flow()` method to be wrapped in Prefect/Dagster later.     |

---

### 10. Next-Step Checklist

1. **Create repo & folder skeleton** (use template above).
2. Add supplied **DDL** for `meta` schema; grant roles.
3. Write first **pandera schema** for `citizens`.
4. Implement **ingest.py** and unit-test checksum logic.
5. Load July 2025 sample files through full pipeline; tag `load_2025-07`.
6. Document manual runbook in `README.md`.

---

Need deeper guidance on any script, Prefect migration, or Power BI model? Just let me know.
